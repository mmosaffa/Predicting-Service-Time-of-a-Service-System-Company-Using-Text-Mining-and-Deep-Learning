{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f20afd4",
   "metadata": {},
   "source": [
    "# Codes for implementing prediction model for forecasting service-time in a repair service company\n",
    "\n",
    "### Author: Mohammad Mosaffa, mohammadmosaffa@ubc.ca\n",
    "The most majority of these codes are in Python, and the minority in SQL Server which include the section of selecting data from dataset.\n",
    "\n",
    "\n",
    "### Section 1, Feature Engieering:\n",
    "In the following codes, the number of laptops that have been in the queue for being repaired is calculated for each record. This feature can significantly influence the service time, and its effectiveness will be discussed comprehensively.\n",
    "\n",
    "As a result, by having time and date of entering for each device, two new variables were created:\n",
    "\n",
    "First, the cumulative number of devices have been in the queue\n",
    "\n",
    "The second is the total number of devices that are collected for a day when a specific device arrives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817117a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df=pd.read_excel('C:/Users/moham/OneDrive/Desktop/Article/New folder (2)/Final_Df.xlsx',na_values=' ')\n",
    "\n",
    "df['Enter_Date']=df['Enter_Date'].agg(pd.Timestamp)\n",
    "df['Exit_Date']=df['Exit_Date'].agg(pd.Timestamp)\n",
    "df['Exit_Date']-df['Enter_Date']\n",
    "\n",
    "df['TotalDays'] = df['Exit_Date']-df['Enter_Date']\n",
    "df[\"Received_in_Day\"] = 0\n",
    "df[\"In_Queue\"] = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    print(i)\n",
    "    counter = i+1\n",
    "    while df.iloc[i,7] >= df.iloc[counter,1]:\n",
    "        if df.iloc[i,7] == df.iloc[counter,1]:\n",
    "            df.iloc[counter,19] = df.iloc[counter,19] + 1\n",
    "        else:\n",
    "            df.iloc[counter,18] = df.iloc[counter,18] + 1\n",
    "        counter = counter + 1\n",
    "        if counter == len(df):\n",
    "            break\n",
    "        \n",
    "df.to_excel('C:/Users/moham/OneDrive/Desktop/Article/New folder (2)/saved_file.xlsx')\n",
    "df.loc[(df['Exit_Date']-df['Enter_Date'])<90]\n",
    "\n",
    "df['TotalDays'] = round(df.iloc[:,6]/(3600*24))\n",
    "\n",
    "df[df['Enter_Year']==2018].iloc[:,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2e781",
   "metadata": {},
   "source": [
    "By coding properly, the name of each days were obtained in order to split a date into name of the day, name of the month, and the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Enter_Date']=df['Enter_Date'].agg(pd.Timestamp)\n",
    "df['Exit_Date']=df['Exit_Date'].agg(pd.Timestamp)\n",
    "df.loc[0,'Enter_Date'].day_name()\n",
    "for i in range(len(df)):\n",
    "    df.loc[i,'Enter_Day_Name'] = df.loc[i,'Enter_Date'].day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f511b8",
   "metadata": {},
   "source": [
    "### Section 2, Obtaining the distribution of the actual service-time:\n",
    "\n",
    "By using fitter from the fitter package, some well-known statistical distributions were fitted to obtain the best distribution with the suitable parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10567bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_context(\"paper\", font_scale = 2)\n",
    "sns.displot(data=dfff.loc[dfff['TotalDays']<=30,'TotalDays'], bins = 30, aspect = 1.5)\n",
    "Days = data=dfff.loc[dfff['TotalDays']<=10000,'TotalDays']\n",
    "Dayss = data=dfff.loc[dfff['TotalDays']<=30,'TotalDays']\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "\n",
    "f = Fitter(Days,\n",
    "           distributions=[\n",
    "            'chi2',\n",
    "            'expon',\n",
    "            'exponpow',\n",
    "            'gamma'])\n",
    "f.fit()\n",
    "\n",
    "f.summary()\n",
    "\n",
    "get_common_distributions()\n",
    "\n",
    "f.fitted_param['expon']\n",
    "f.fitted_pdf['expon']\n",
    "\n",
    "dist = scipy.stats.expon\n",
    "param = (0.0, 4.5623891469594597)\n",
    "X = linspace(0,30, 10)\n",
    "pdf_fitted = dist.pdf(X, *param)\n",
    "plot(X, pdf_fitted, 'o-')\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.title('Accuracy of Approximated Exponential Dist for The Actual Waiting-Time')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Density')\n",
    "sns.kdeplot(dfff.loc[dfff['TotalDays']<=30,'TotalDays'], shade=True, color='olive')\n",
    "plot(X, pdf_fitted, 'o-')\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.legend(['Actual Density','Predicted Exponential'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466ac7d",
   "metadata": {},
   "source": [
    "### Section 3, Preprocessing the dataset:\n",
    "In this section, all columns except the text on was investigated in order to transform them into a form which is suitable for predictive model.\n",
    "\n",
    "This section's main challenge is cleaning the **Estimation** column. It must be numeric since each device is a repairing cost guess. However, it had text, numbers, and symbols. Also, some records had an interval estimated cost, which the average of them was calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming time to the number\n",
    "from datetime import *\n",
    "df.iloc[:,5]=df.iloc[:,5].agg(pd.Timestamp)\n",
    "Time = np.zeros((len(df),1))\n",
    "for i in range(len(df)):\n",
    "    Time[i] = df.iloc[i,5].hour + (df.iloc[i,5].minute/60)\n",
    "df.iloc[:,5] = Time \n",
    "\n",
    "#Normalazing the Brand column\n",
    "df.iloc[:,6] = df.iloc[:,6].agg(lambda x: x.replace(' ','').lower())\n",
    "\n",
    "#Normalazing the Model column\n",
    "df.iloc[:,7] = df.iloc[:,7].apply(str)\n",
    "df.iloc[:,7] = df.iloc[:,7].agg(lambda x: x.replace(' ','').upper())\n",
    "\n",
    "#Normalazing the Estimated column\n",
    "dfest=df['Estimated_Price'].copy()\n",
    "\n",
    "dfest2 = dfest.copy()\n",
    "dfest = dfest.apply(str)\n",
    "dfest2 = dfest.apply(str)\n",
    "\n",
    "for i in range(len(dfest)):\n",
    "    print(i)\n",
    "    if len([int(s) for s in re.findall(r'\\b\\d+\\b', dfest[i])]) == 0:\n",
    "        dfest2[i] = 0\n",
    "    else:\n",
    "        dfest2[i] = sum([int(s) for s in re.findall(r'\\b\\d+\\b', dfest[i])])/len([int(s) for s in re.findall(r'\\b\\d+\\b', dfest[i])])\n",
    "    \n",
    "dfest=df['Estimated_Price'] = dfest2\n",
    "\n",
    "plt.hist(dfest2, bins=range(0,30000000,1000000), edgecolor=\"black\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6bcd95",
   "metadata": {},
   "source": [
    "#### Section 4, Preprocesing, Text:\n",
    "In this section, first, the text was normalized, then by creating a matrix of words similarities, an aautomatic approach for currecting misspelling words was developed, and finally, by eliminating words had a few repeated in the dataset, others were transformed into numeric variables by using Bag of Words (BOW) techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working on text column\n",
    "diag = df['Explanation'].apply(str).copy()\n",
    "diag2 = diag.copy()\n",
    "re.sub('[^\\w\\s]',\"\",diag[2])\n",
    "re.findall('\\w',diag[2])\n",
    "\n",
    "re.findall('\\w\\s',diag[2])#Find word characters followed by a white space\n",
    "re.findall('\\d+\\$',diag[2])\n",
    "\n",
    "#Eleminating all foreign charachters like english and numbers\n",
    "for i in range(len(diag)):\n",
    "    diag[i] = re.sub(\"[a-zA-Z0-9]+\", \"\",diag2[i])\n",
    "    diag[i] = re.sub('[^\\w\\s]', \"\",diag[i])\n",
    "\n",
    "#Normalizing two kind of 'ye' in farsi    \n",
    "for i in range(len(diag)):\n",
    "    diag[i] = diag[i].replace('ي', 'ی')\n",
    "\n",
    "#Removing the stopwords in farsi  \n",
    "prstop=pd.read_excel('C:/Users/moham/OneDrive/Desktop/Article/Dataset/stopwords.xlsx') \n",
    "Row_list =[]\n",
    "for i in range(len(prstop)):\n",
    "    Row_list.append(prstop.iloc[i,0])\n",
    "    \n",
    "words = diag[1].split()\n",
    "for word in words:\n",
    "    if word in Row_list:\n",
    "        print(word)\n",
    "        words.remove(word)\n",
    "words = ' '.join(words)\n",
    "\n",
    "#Removing some bold mistakes in the dataset in writing some explanations\n",
    "diag3 = diag2.copy()\n",
    "for i in range(len(diag3)):\n",
    "    words = diag3[i].split()\n",
    "    for word in words:\n",
    "        if word in Row_list or word == 'نميشود' or word == 'ميشود' or word == 'و' or word == 'گي' or word == 'در':\n",
    "            words.remove(word)\n",
    "    diag3[i] = ' '.join(words)\n",
    "    \n",
    "\n",
    "#Removing some bold mistakes in the dataset in writing some explanations part 2\n",
    "z1 = []\n",
    "z2 = []\n",
    "z3 = []\n",
    "z4 = []\n",
    "z5 = []\n",
    "z = ['ا','آ']\n",
    "for i in range(len(diag3)):\n",
    "    z1 = re.findall('ال دي',diag3.iloc[i])\n",
    "    z2 = re.findall('ال سي دي',diag3.iloc[i])\n",
    "    z3 = re.findall('مادر برد',diag3.iloc[i])\n",
    "    z4 = re.findall('يو اس',diag3.iloc[i])\n",
    "    z5 = re.findall('ال سي ذي',diag3.iloc[i])\n",
    "    re.sub(z[0],z[1],diag.iloc[i])\n",
    "    if z1 != []:\n",
    "        diag3.iloc[i] = re.sub(z1[0],'الايدي',diag3.iloc[i])\n",
    "    z1 = []\n",
    "    if z2 != []:\n",
    "        diag3.iloc[i] = re.sub(z2[0],'السيدي',diag3.iloc[i])\n",
    "    z2 = []\n",
    "    if z3 != []:\n",
    "        diag3.iloc[i] = re.sub(z3[0],'مادربرد',diag3.iloc[i])\n",
    "    z3 = []\n",
    "    if z4 != []:\n",
    "        diag3.iloc[i] = re.sub(z4[0],'يواسبس',diag3.iloc[i])\n",
    "    z4 = []\n",
    "    if z5 != []:\n",
    "        diag3.iloc[i] = re.sub(z5[0],'السيدي',diag3.iloc[i])\n",
    "    z5 = []\n",
    "\n",
    "#Counting the number of each words in dataset\n",
    "numberofwords = diag3.str.split(expand=True).stack().value_counts()\n",
    "ExpWords = numberofwords[numberofwords>=20]\n",
    "\n",
    "#Removing some alone charachter in the text\n",
    "a = ['ا','آ','ب','پ','ت','ث','ج','چ','ح','خ','د','ذ','ر','ز','ژ','س','ش','ص','ض','ط','ظ','ع','غ','ک','گ','ل','م','ن','و','ه','ی','ئ','ي','ف','ق',' ']\n",
    "for i in range(len(diag3)):\n",
    "    words = diag3[i].split()\n",
    "    for word in words:\n",
    "        if word in a:\n",
    "            words.remove(word)\n",
    "    diag3[i] = ' '.join(words)   \n",
    "    \n",
    "numberofwords.index\n",
    "numberofwords.to_excel('C:/Users/moham/OneDrive/Desktop/Article/Dataset/matrix.xlsx')\n",
    "numberofwords2=pd.read_excel('C:/Users/moham/OneDrive/Desktop/Article/Dataset/matrix.xlsx') \n",
    "list(numberofwords2.iloc[9,0])[0] in list(numberofwords2.iloc[9,0])\n",
    "\n",
    "numberofwords2back = numberofwords2.copy()\n",
    "count = 0\n",
    "for i in range(len(numberofwords2)):\n",
    "    print(i)\n",
    "    word1 = list(numberofwords2.iloc[i,0])\n",
    "    for j in range(len(numberofwords2)):\n",
    "        word2 = list(numberofwords2.iloc[j,0])\n",
    "        for k in range(len(word1)):\n",
    "            if word1[k] in word2:\n",
    "                count = count + 1\n",
    "        numberofwords2.iloc[i,j+1] = count/len(word1)\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb36483",
   "metadata": {},
   "source": [
    "In the following section, codes were programmed in order to create a misspelling matrix gives similarity precentage of each two words. By sorting words based on repeated in the dataset, a word with high similarity with the frequent form of that can be replaced. In this paper, words with similarity of 90% or higher with original form were replaced.\n",
    "\n",
    "It should be mentioned that this approach is a semi-intelligent it would work better by supervising a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67756ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missplelling words deal\n",
    "numberofwords2back = numberofwords2.copy()\n",
    "\n",
    "count = 0\n",
    "for i in range(len(numberofwords2)):\n",
    "    print(i)\n",
    "    word1 = list(numberofwords2.iloc[i,0])\n",
    "    for j in range(len(numberofwords2)):\n",
    "        word2 = list(numberofwords2.iloc[j,0])\n",
    "        if len(word1) > len(word2):\n",
    "            lenn = len(word2)\n",
    "        else:\n",
    "            lenn = len(word1)\n",
    "        for k in range(0,lenn):\n",
    "            if word1[k] == word2[k]:\n",
    "                count = count + 1\n",
    "        numberofwords2.iloc[i,j+1] = count/lenn\n",
    "        count = 0\n",
    "        \n",
    "numberofwords2.to_excel('C:/Users/moham/OneDrive/Desktop/Article/Dataset/matrix2.xlsx')\n",
    "\n",
    "numberofwords2back = numberofwords2back.drop(numberofwords2back.index[[29, 40, 45, 51, 66, 76, 80, 92, 98, 109]])\n",
    "numberofwords2back = numberofwords2back.reset_index()\n",
    "del df['index']\n",
    "numberofwords2back = numberofwords2back.drop(numberofwords2back.index[[58]])\n",
    "numberofwords2back = numberofwords2back.reset_index()\n",
    "del df['index']\n",
    "numberofwords2back.loc[58,numberofwords2back.iloc[58,:] == 1]\n",
    "برداشته برد\n",
    "numberofwords2.loc[15,numberofwords2.iloc[15,:] == 1]\n",
    "numberofwords2back.loc[8,numberofwords2back.iloc[8,:] == 1].index[15]\n",
    "\n",
    "diag[49]   \n",
    "\n",
    "numberofwords2back.loc[8,numberofwords2back.iloc[8,:] == 1].index[15] in diag[49]\n",
    "\n",
    "\n",
    "diag4 = diag3.copy()\n",
    "listt = []\n",
    "for i in range(len(diag4)):\n",
    "    print(i)\n",
    "    for t in range(len(numberofwords2back)):\n",
    "        for k in range(len(numberofwords2back.loc[t,numberofwords2back.iloc[t,:] == 1])):\n",
    "            if numberofwords2back.loc[t,numberofwords2back.iloc[t,:] == 1].index[k] in diag4[i]:\n",
    "                for j in range(len(diag4[i].split())):\n",
    "                    if diag4[i].split()[j] == numberofwords2back.loc[t,numberofwords2back.iloc[t,:] == 1].index[k]:    \n",
    "                        word = diag4[i].split()[j].split(numberofwords2back.loc[t,numberofwords2back.iloc[t,:] == 1].index[k])\n",
    "                        listt.append(word[1])\n",
    "                        listt.append(numberofwords2back.loc[t,numberofwords2back.iloc[t,:] == 1].index[k])\n",
    "                    else:\n",
    "                        listt.append(diag4[i].split()[j])\n",
    "                diag4[i] = ' '.join(listt)\n",
    "                listt = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0395fe",
   "metadata": {},
   "source": [
    "Eliminating extranous words from the dataset and replace suspicious words with the confidence form of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "diag4 = diag3.copy()\n",
    "listt = []\n",
    "for j in range(len(diag4)):\n",
    "    for i in range(0,100):\n",
    "        listt = []\n",
    "        if (len(intersection(diag4[j].split(),numberofwords2back.loc[i,numberofwords2back.iloc[i,:] == 1].index[1:])) > 0) \n",
    "        and (len(diag4[j].split(numberofwords2back.loc[i,numberofwords2back.iloc[i,:] == 1].index[0])) > 1) \n",
    "        and (numberofwords2back.loc[i,numberofwords2back.iloc[i,:] == 1].index[0] not in diag4[j].split()) :\n",
    "            \n",
    "            print(intersection(diag4[j].split(),numberofwords2back.loc[i,numberofwords2back.iloc[i,:] == 1].index[1:]))\n",
    "            print(j)\n",
    "            print(numberofwords2back.loc[i,numberofwords2back.iloc[i,:] == 1].index[0])\n",
    "            listt = (diag4[j].split(numberofwords2back.loc[i,numberofwords2back.iloc[i,:] == 1].index[0])) \n",
    "            print(listt)\n",
    "            listt.append(numberofwords2back.loc[i,numberofwords2back.iloc[i,:] == 1].index[0])\n",
    "            print(listt)\n",
    "            listt = ' '.join(listt) \n",
    "            print(listt)\n",
    "            diag4[j] = listt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20b080",
   "metadata": {},
   "source": [
    "Transforming text into numeric by using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db6110",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag4 = diag4.apply(str)\n",
    "Words = ''\n",
    "for i in diag4:\n",
    "    word = ' '.join(i.split())\n",
    "    Words = Words +' ' + word\n",
    "\n",
    "FinalWords = diag4.agg(lambda x:' '.join(x))\n",
    "\n",
    "z = diag4[diag4 != 'nan']\n",
    "\n",
    "word_counts = diag4[diag4 != 'nan'].str.split(expand=True).stack().value_counts()\n",
    "\n",
    "Final_Words = word_counts[word_counts>=20]\n",
    "\n",
    "\n",
    "dff = df.copy()\n",
    "allfeatures = np.zeros((dff.shape[0],Final_Words.shape[0]))\n",
    "dff['Explanation'] = diag4\n",
    "\n",
    "dff[dff['Explanation'] == 'nan']['Explanation'] = ' '\n",
    "\n",
    "for i in range(len(dff)):\n",
    "    if dff.iloc[i,8] == 'nan':\n",
    "        dff.iloc[i,8] = ' '\n",
    "        \n",
    "dff['Explanation'] = dff['Explanation'].apply(str)\n",
    "for i in np.arange(Final_Words.shape[0]):\n",
    "    allfeatures[:,i] = dff['Explanation'].agg(lambda x:len(re.search(Final_Words.index[i],x)))\n",
    "\n",
    "allfeatures.to_excel('C:/Users/moham/OneDrive/Desktop/Article/Dataset/allfeatures.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64ca88",
   "metadata": {},
   "source": [
    "Concatenating all features into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ce19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "allfeatures[]\n",
    "allfeatures.iloc[:,allfeatures.iloc[0,:].values == 5]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "Complete_data = pd.concat([dff.iloc[:,[2,3,4,5,6,7,8,9,10,11,14]],allfeatures],1)\n",
    "\n",
    "Complete_data = pd.concat([dff.iloc[:,[2,3,4,5,6,7,8,9,10,11,14]]],1)\n",
    "\n",
    "Complete_data.to_excel('C:/Users/moham/OneDrive/Desktop/Article/Dataset/PreparedData.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83b11a",
   "metadata": {},
   "source": [
    "### Section 4, Prepared final data for predictive models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be00380",
   "metadata": {},
   "outputs": [],
   "source": [
    "Complete_data = pd.read_csv('/content/PreparedData2.csv',na_values=' ') \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "plt.title('Without In_Queue number, Without adding weight')\n",
    "plt.hist(Complete_data['DaysToFinished'], bins=range(0,11,1), edgecolor=\"black\", color='purple')\n",
    "\n",
    "Complete_data2 = Complete_data.copy()\n",
    "\n",
    "Complete_data2 = Complete_data2[Complete_data2['DaysToFinished'] <= 8]\n",
    "\n",
    "#Complete_data2 = Complete_data2[Complete_data2['Explanation'].notna()]\n",
    "\n",
    "X = Complete_data2.loc[:, Complete_data2.columns != 'DaysToFinished']\n",
    "\n",
    "X = X.loc[:, X.columns != 'Explanation']\n",
    "\n",
    "Y = Complete_data2['DaysToFinished']\n",
    "\n",
    "enc=LabelEncoder()\n",
    "\n",
    "enc.fit(X['Enter_Month'])\n",
    "X['Enter_Month'] = enc.transform(X['Enter_Month'])\n",
    "\n",
    "enc.fit(X['Enter_Day_Name'])\n",
    "X['Enter_Day_Name'] = enc.transform(X['Enter_Day_Name'])\n",
    "\n",
    "enc.fit(X['Enter_Year'])\n",
    "X['Enter_Year'] = enc.transform(X['Enter_Year'])\n",
    "\n",
    "enc.fit(X['Brand'])\n",
    "X['Brand'] = enc.transform(X['Brand'])\n",
    "\n",
    "enc.fit(X['Model'])\n",
    "X['Model'] = enc.transform(X['Model'])\n",
    "\n",
    "enc.fit(Y)\n",
    "y = enc.transform(Y)\n",
    "\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a545c",
   "metadata": {},
   "source": [
    "#### Variable selection and ranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6629c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Chi2 method\n",
    "X = X.loc[:, X.columns != 'Model']\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  #print 10 best features\n",
    "\n",
    "\n",
    "#Decision Tree variables selection\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "#Selecting top 100 variables from the dataset\n",
    "X.shape\n",
    "X_new = SelectKBest(chi2, k=100).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42076f8f",
   "metadata": {},
   "source": [
    "As we know, categorical variables may be treated by number ranking. As a result, it would be reasonable to transform categorical variables into 0 and 1 form by using dummy variables techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X,pd.get_dummies(X['Enter_Month'])],1)\n",
    "X = pd.concat([X,pd.get_dummies(X['Enter_Day_Name'])],1)\n",
    "X = pd.concat([X,pd.get_dummies(X['Enter_Year'])],1)\n",
    "X = pd.concat([X,pd.get_dummies(X['Brand'])],1)\n",
    "#X = pd.concat([X,pd.get_dummies(X['Model'])],1)\n",
    "\n",
    "X = X.loc[:, X.columns != 'Enter_Month']\n",
    "X = X.loc[:, X.columns != 'Enter_Day_Name']\n",
    "X = X.loc[:, X.columns != 'Enter_Year']\n",
    "X = X.loc[:, X.columns != 'Brand']\n",
    "X = X.loc[:, X.columns != 'Model']\n",
    "\n",
    "X.shape[1]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15c5e0",
   "metadata": {},
   "source": [
    "Data selection into test and train. Also, all variebales were normalized by using MinMax scaler technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a106dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15) \n",
    "\n",
    "X_train_N = X_train.copy()\n",
    "X_test_N = X_test.copy()\n",
    "\n",
    "X_train_N = X_train_N.loc[:, X_train_N.columns != 'In_Queue']\n",
    "X_train_N = X_train_N.loc[:, X_train_N.columns != 'Received_in_Day']\n",
    "\n",
    "X_test_N = X_test_N.loc[:, X_test_N.columns != 'In_Queue']\n",
    "X_test_N = X_test_N.loc[:, X_test_N.columns != 'Received_in_Day']\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "mms.fit(X_train)\n",
    "X_train = mms.transform(X_train)\n",
    "mms.fit(X_test)\n",
    "X_test = mms.transform(X_test)\n",
    "\n",
    "mms.fit(X_train_N)\n",
    "X_train_N = mms.transform(X_train_N)\n",
    "mms.fit(X_test_N)\n",
    "X_test_N = mms.transform(X_test_N)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb43af",
   "metadata": {},
   "source": [
    "### Section 5, Applying machine learning methods (Regression):\n",
    "By using cross-validation techniques, optmized hyperparameters of **RandomForestRegressor** and **GradientBoostingRegressor** were obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc19e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "regressor1 = LinearRegression()\n",
    "regressor2 = PoissonRegressor()\n",
    "regressor3 = BayesianRidge()\n",
    "regressor4 = SVR(kernel = 'rbf')\n",
    "regressor5 = RandomForestRegressor(n_estimators=500, max_features='auto', random_state=0 , max_depth = 20)\n",
    "regressor6 = GradientBoostingRegressor(loss='squared_error', n_estimators = 200, max_depth = 20, max_features = 'auto')\n",
    "\n",
    "regressor1.fit(X_train, y_train)\n",
    "regressor2.fit(X_train, y_train)\n",
    "regressor3.fit(X_train, y_train)\n",
    "regressor4.fit(X_train, y_train)\n",
    "regressor5.fit(X_train, y_train)\n",
    "regressor6.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = regressor1.predict(X_test)\n",
    "y_pred2 = regressor2.predict(X_test)\n",
    "y_pred3 = regressor3.predict(X_test)\n",
    "y_pred4 = regressor4.predict(X_test)\n",
    "y_pred5 = regressor5.predict(X_test)\n",
    "y_pred6 = regressor6.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49adb8ef",
   "metadata": {},
   "source": [
    "### Section 6, Applying deep learning method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cdf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#Model one\n",
    "f = lambda a: (abs(a)+a)/2 \n",
    "input_layer = Input(shape=(X.shape[1],))\n",
    "dense_layer_1 = Dense(512, activation='relu')(input_layer)\n",
    "dense_layer_2 = Dense(128, activation='relu')(dense_layer_1)\n",
    "dense_layer_3 = Dense(64, activation='relu')(dense_layer_2)\n",
    "output = (Dense(1)(dense_layer_3))\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=256, epochs=30, verbose=1, validation_split=0.1)\n",
    "\n",
    "pred_train = model.predict(X_train)\n",
    "print(np.sqrt(mean_squared_error(y_train,pred_train)))\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test,f(pred))))\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print((mean_absolute_error(y_test,pred)))\n",
    "\n",
    "\n",
    "#Model two\n",
    "f=lambda a: (abs(a)+a)/2 \n",
    "input_layer = Input(shape=(X.shape[1],))\n",
    "dense_layer_1 = Dense(256, activation='relu')(input_layer)\n",
    "dense_layer_2 = Dense(64, activation='relu')(dense_layer_1)\n",
    "dense_layer_3 = Dense(32, activation='relu')(dense_layer_2)\n",
    "output = (Dense(1)(dense_layer_3))\n",
    "\n",
    "model2 = Model(inputs=input_layer, outputs=output)\n",
    "model2.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "\n",
    "history2 = model2.fit(X_train, y_train, batch_size=256, epochs=30, verbose=1, validation_split=0.1)\n",
    "\n",
    "pred_train2 = model2.predict(X_train)\n",
    "print(np.sqrt(mean_squared_error(y_train,pred_train2)))\n",
    "\n",
    "pred2 = model2.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test,pred2)))\n",
    "print((mean_absolute_error(y_test,(pred2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6511585f",
   "metadata": {},
   "source": [
    "### Section 7, Visualization and comparing methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "print(mean_absolute_error(y_train,pred_train))\n",
    "\n",
    "pred = f(model.predict(X_test))\n",
    "pred2 = np.floor(f(model2.predict(X_test)))\n",
    "\n",
    "print(mean_absolute_error(y_test,pred))\n",
    "print(((8-mean_absolute_error(y_test,pred))/8)*100)\n",
    "\n",
    "print(mean_absolute_error(y_test,pred2))\n",
    "print(((8-mean_absolute_error(y_test,pred2))/8)*100)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.title('Histogram of the GradientBoostingRegressor prediction with actual labels')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number')\n",
    "#plt.hist(pred, bins=range(0,10,1), edgecolor=\"black\", color='red', alpha=0.5)\n",
    "plt.hist(y_pred6, bins=range(0,10,1), edgecolor=\"black\", color='blue', alpha=0.5)\n",
    "plt.hist(y_test, bins=range(0,10,1), edgecolor=\"black\", color='red', alpha=0.5)\n",
    "\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.legend(['Predicted with GradientBoostingRegressor','Actual'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "lenn = 50\n",
    "f=lambda a: (abs(a)+a)/2 \n",
    "l = np.zeros((8*lenn,2))\n",
    "l = pd.DataFrame(l)\n",
    "counter = 0\n",
    "for i in range(lenn):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15) \n",
    "  mms.fit(X_test)\n",
    "  X_test = mms.transform(X_test)\n",
    "  pred = f(model.predict(X_test))\n",
    "  l.iloc[counter,0] = ((8 - mean_absolute_error(y_test,f(pred)))/8)*100\n",
    "  l.iloc[counter,1] = 'DL_512_Nodes'\n",
    "  counter = counter + 1\n",
    "  pred2 = f(model2.predict(X_test))\n",
    "  l.iloc[counter,0] = ((8 - mean_absolute_error(y_test,f(pred2)))/8)*100\n",
    "  l.iloc[counter,1] = 'DL_256_Nodes'\n",
    "  counter = counter + 1\n",
    "  y_pred1 = regressor1.predict(X_test)\n",
    "  l.iloc[counter,0] = ((8 - mean_absolute_error(y_test,f(y_pred1)))/8)*100\n",
    "  l.iloc[counter,1] = 'LinearRegression'\n",
    "  counter = counter + 1\n",
    "  y_pred2 = regressor2.predict(X_test)\n",
    "  l.iloc[counter,0] = ((8 - mean_absolute_error(y_test,f(y_pred2)))/8)*100\n",
    "  l.iloc[counter,1] = 'PoissonRegressor'\n",
    "  counter = counter + 1\n",
    "  y_pred3 = regressor3.predict(X_test)\n",
    "  l.iloc[counter,0] = ((8 - mean_absolute_error(y_test,f(y_pred3)))/8)*100\n",
    "  l.iloc[counter,1] = 'BayesianRidge'\n",
    "  counter = counter + 1\n",
    "  y_pred4 = regressor4.predict(X_test)\n",
    "  l.iloc[counter,0] = ((8 - mean_absolute_error(y_test,f(y_pred4)))/8)*100\n",
    "  l.iloc[counter,1] = 'SVR'\n",
    "  counter = counter + 1\n",
    "  y_pred5 = regressor5.predict(X_test)\n",
    "  l.iloc[counter,0] = ((8 - mean_absolute_error(y_test,f(y_pred5)))/8)*100\n",
    "  l.iloc[counter,1] = 'RandomForestRegressor'\n",
    "  counter = counter + 1\n",
    "  y_pred6 = regressor6.predict(X_test)\n",
    "  l.iloc[counter,0] = ((8 - mean_absolute_error(y_test,f(y_pred6)))/8)*100\n",
    "  l.iloc[counter,1] = 'GradientBoostingRegressor'\n",
    "  counter = counter + 1\n",
    "\n",
    "sns.boxplot(x=l.iloc[:,0], y=l.iloc[:,1], linewidth=5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.subplot(5, 2, 1)\n",
    "plt.hist(pred, bins=range(0,10,1), edgecolor=\"black\", color='green', alpha=0.5)\n",
    "plt.legend(['DL_512_Nodes'])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number')\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.subplot(5, 2, 2)\n",
    "plt.hist(pred2, bins=range(0,10,1), edgecolor=\"black\", color='green', alpha=0.5)\n",
    "plt.legend(['DL_256_Nodes'])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number')\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.subplot(5, 2, 3)\n",
    "plt.hist(y_pred1, bins=range(0,10,1), edgecolor=\"black\", color='red', alpha=0.5)\n",
    "plt.legend(['LinearRegression'])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number')\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.subplot(5, 2, 4)\n",
    "plt.hist(y_pred2, bins=range(0,10,1), edgecolor=\"black\", color='red', alpha=0.5)\n",
    "plt.legend(['PoissonRegressor'])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number')\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.subplot(5, 2, 5)\n",
    "plt.hist(y_pred3, bins=range(0,10,1), edgecolor=\"black\", color='black', alpha=0.5)\n",
    "plt.legend(['BayesianRidge'])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number')\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.subplot(5, 2, 6)\n",
    "plt.hist(y_pred4, bins=range(0,10,1), edgecolor=\"black\", color='black', alpha=0.5)\n",
    "plt.legend(['SVR'])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number')\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.subplot(5, 2, 7)\n",
    "plt.hist(y_pred5, bins=range(0,10,1), edgecolor=\"black\", color='black', alpha=0.5)\n",
    "plt.legend(['RandomForestRegressor'])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number')\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.subplot(5, 2, 8)\n",
    "plt.hist(y_pred6, bins=range(0,10,1), edgecolor=\"black\", color='green', alpha=0.5)\n",
    "plt.legend(['GradientBoostingRegressor'])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number')\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.subplot(5, 2, 10)\n",
    "plt.hist(y_test, bins=range(0,10,1), edgecolor=\"black\", color='purple', alpha=0.5)\n",
    "plt.legend(['Actual Data'])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number')\n",
    "plt.grid(linestyle='-', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18fbb5",
   "metadata": {},
   "source": [
    "### Section 8, Satatistics test for predicted service time with actual service time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "f_oneway(df[df.iloc[:,1] == 'PoissonRegressor'].iloc[:,0], df[df.iloc[:,1] == 'LinearRegression'].iloc[:,0])\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "fvalue, pvalue = stats.f_oneway(df[df.iloc[:,1] == 'DL_512_Nodes'].iloc[:,0], \n",
    "                                df[df.iloc[:,1] == 'DL_256_Nodes'].iloc[:,0],\n",
    "                                df[df.iloc[:,1] == 'LinearRegression'].iloc[:,0],\n",
    "                                df[df.iloc[:,1] == 'PoissonRegressor'].iloc[:,0],\n",
    "                                df[df.iloc[:,1] == 'BayesianRidge'].iloc[:,0],\n",
    "                                df[df.iloc[:,1] == 'SVR'].iloc[:,0],\n",
    "                                df[df.iloc[:,1] == 'RandomForestRegressor'].iloc[:,0],  \n",
    "                                df[df.iloc[:,1] == 'GradientBoostingRegressor'].iloc[:,0])\n",
    "\n",
    "print(fvalue, pvalue)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "tukey = pairwise_tukeyhsd(endog=df.iloc[:,0],\n",
    "                          groups=df.iloc[:,1],\n",
    "                          alpha=0.05)\n",
    "\n",
    "print(tukey)\n",
    "\n",
    "\n",
    "fvalue, pvalue = stats.f_oneway(df[df.iloc[:,1] == 'SVR'].iloc[:,0], \n",
    "                                df[df.iloc[:,1] == 'RandomForestRegressor'].iloc[:,0])\n",
    "\n",
    "print(fvalue, pvalue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
